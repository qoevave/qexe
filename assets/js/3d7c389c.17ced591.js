"use strict";(self.webpackChunkqexe_website=self.webpackChunkqexe_website||[]).push([[9969],{8995:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>c});var s=i(4848),o=i(8453);const t={sidebar_position:4,custom_edit_url:null,description:"Creating your evaluation scenes..."},d="Evaluation Scenes",a={id:"getting-started/eval-scenes",title:"Evaluation Scenes",description:"Creating your evaluation scenes...",source:"@site/docs/getting-started/eval-scenes.mdx",sourceDirName:"getting-started",slug:"/getting-started/eval-scenes",permalink:"/qexe/docs/getting-started/eval-scenes",draft:!1,unlisted:!1,editUrl:null,tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,custom_edit_url:null,description:"Creating your evaluation scenes..."},sidebar:"tutorialSidebar",previous:{title:"Scene Types",permalink:"/qexe/docs/getting-started/scene-types"},next:{title:"Download",permalink:"/qexe/docs/downloading"}},r={},c=[{value:"Making an Evaluation Scene",id:"making-an-evaluation-scene",level:2},{value:"Object-based Audio + CGI",id:"object-based-audio--cgi",level:3},{value:"Object-based + 360 Video",id:"object-based--360-video",level:3},{value:"Ambisonics audio + 360 Video",id:"ambisonics-audio--360-video",level:3},{value:"Ambisoncs audio + CGI",id:"ambisoncs-audio--cgi",level:3}];function l(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"evaluation-scenes",children:"Evaluation Scenes"}),"\n",(0,s.jsx)(n.h2,{id:"making-an-evaluation-scene",children:"Making an Evaluation Scene"}),"\n",(0,s.jsxs)(n.p,{children:["The QExE tool is used to define evaluation scene entires. We'll work through a few definitions and examples on how these scenes are constructed.\nScenes are defined in the ",(0,s.jsx)(n.a,{href:"/qexe/docs/getting-started/config-file",children:"configuration file"})," using a set of identifiers, parameters, and audio and visual material."]}),"\n",(0,s.jsx)(n.h3,{id:"object-based-audio--cgi",children:"Object-based Audio + CGI"}),"\n",(0,s.jsx)(n.p,{children:"Here's an example scene configuration for a CGI scene. The evaluation scene uses individual audio files which are associated with audio objects in virtual reality, and are rendered using an object-based audio workflow."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-JSON",children:'"Scenes" : [\n    {\n        "SceneID" : "Scene_stereo_bongos_interaction",\n        "DegreeOfFreedom" : 6,\n        "UnitySceneID" : "_bongos_demo",\n        "VisualStimuli" : "Simulated",\n        "Dimensions" : [ 30.0, 8.0, 30.0 ],\n        "NumberOfAudioFiles" : 2,\n        "AudioAmbisonicsFolder" : "<null>",\n        "AudioObjectsFolder" : "../../Content/audio/bongosScene",\n        "AudioObjectsrouting" : [\n            {\n                "PathToFile" : "bongos_L.wav",\n                "InputChannel" : 0,\n                "Autoplay" : 1\n            },\n            {\n                "PathToFile" : "bongos_R.wav",\n                "InputChannel" : 1,\n                "Autoplay" : 1\n            },\n        ]\n    }\n]\n'})}),"\n",(0,s.jsx)(n.p,{children:"Let's break it down..."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"SceneID"})," : A ",(0,s.jsx)(n.code,{children:"<string>"})," entry which is used only for data management and result file legends. This can be anything the user wishes, provided there are ",(0,s.jsx)(n.em,{children:(0,s.jsx)(n.strong,{children:"no spaces"})}),". It is recommended to provide a ",(0,s.jsx)(n.code,{children:"SceneID"})," which reflects a mixture of the type of audio and visual material, and maybe even the scene or evaluation task."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"DegreesOfFreedom"})," : An ",(0,s.jsx)(n.code,{children:"<integer>"})," entry or either 3 or 6. This is a placeholder for furture development."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"UnitySceneID"})," : A ",(0,s.jsx)(n.code,{children:"<string>"})," entry used to inform the Unity client which Unity scene should be loaded. The entry here needs to be ",(0,s.jsx)(n.strong,{children:"exactly the same"})," as the saved Unity scene in your Unity."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"VisualStimuli"})," : A ",(0,s.jsx)(n.code,{children:"<string>"})," entry used by the QExE host to determine which further config entries are of interest. The options are either ",(0,s.jsx)(n.code,{children:"Simulated"})," or ",(0,s.jsx)(n.code,{children:"360video"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"Dimensions"})," : An array entry depicting the size of the virtual scene. These values are needed to normalize the incoming positional information for the user, and position information between 1 and 0."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"NumberOfAudioFiles"})," : An int entry used to specify the number of audio files."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"AudioAmbisonicsFolder"})," : A string entry to point the QExE host to a folder containing the multichannel audio file used for multichannel audio WebGL2RenderingContext, i.e., Ambisonics."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"AudioObjectsFolder"})," : A string entry to point the QExE host to a folder containing all the mono audio files used for object-based rendering."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"AudioObjectsRouting"})," : An array entry of nested data regarding specific audio files. For each audio file, we provide the information of the ",(0,s.jsx)(n.code,{children:"PathToFile"}),", ",(0,s.jsx)(n.code,{children:"InputChannel"}),", and ",(0,s.jsx)(n.code,{children:"Autoplay"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"object-based--360-video",children:"Object-based + 360 Video"}),"\n",(0,s.jsx)(n.p,{children:"An example using a 360 which has been augments by audio objects with the object rendering pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-JSON",children:'"Scenes" : [\n    {\n        "SceneID" : "Scene_sax_video_audio_object",\n        "DegreeOfFreedom" : 3,\n        "UnitySceneID" : "_videoPlayer",\n        "VisualStimuli" : "360Video",\n        //highlight-start\n        "VideoFiles" : [\n            {\n                "PathToFile" : "../../Content/video/saxophoneBusking.mp4",\n                "VideoID" : 0\n            }\n        ],\n        //highlight-end\n        "Dimensions" : [ 5, 5, 5 ],\n        "NumberOfAudioFiles" : 1,\n        "AudioAmbisonicsFolder" : "<null>",\n        "AudioObjectsFolder" : "../../Content/audio/saxophoneScene",\n        "AudioObjectsrouting" : [\n            {\n                "PathToFile" : "bongos_L.wav",\n                "InputChannel" : 0,\n                "Autoplay" : 1\n            },\n        ]\n    }\n]\n'})}),"\n",(0,s.jsx)(n.p,{children:"Here have changed a few values, and added a couple of new entries..."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"UnitySceneID"})," : This has now been changed to ",(0,s.jsx)(n.code,{children:"_videoPlayer"}),", which will load the QExE client videoPlayer scene."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"VisualStimuli"})," : This is now been changed to ",(0,s.jsx)(n.code,{children:"360video"})," as a flag for the QExE host."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Dimensions"})," : This has now been changed to a box shape. The dimensions specified depends on the intended perceptual distance you might like to place your augmented audio Object."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"VideoFiles"})," : An array entry specficying the ",(0,s.jsx)(n.code,{children:"PathToFile"})," and a given ",(0,s.jsx)(n.code,{children:"VideoID"})," index per video file."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"I have a question...",type:"info",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.em,{children:(0,s.jsx)(n.strong,{children:"Question"})}),": Why can we have an array entry to specify multiple videos for a single scene? We couldn't do this for the CGI content example, and we can't play multiple videos at the same time..."]})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.em,{children:(0,s.jsx)(n.strong,{children:"Answer"})}),": A scene that is recorded via a 360 camera doesn't change in terms of the content that is recorded. However, it CAN change in terms of encoding, resolution, or frame-rate, for example. Therefore, the content is classified as the same scene. If we recorded the same scene twice, but maybe had subtely different content with maybe more people or cars in the background this is essentially a different scene as different content is presented to the subject."]}),"\n",(0,s.jsxs)(n.p,{children:["The method above in an efficient way of creating test items for ",(0,s.jsx)(n.a,{href:"/qexe/docs/features/methods",children:"ACR"})," video quality evaluation, where all other parameters would remain the same."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ambisonics-audio--360-video",children:"Ambisonics audio + 360 Video"}),"\n",(0,s.jsx)(n.p,{children:"An example using the multichannel rendering pipline with Ambisoncis audio and 360 video..."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-JSON",children:'"Scenes" : [\n    {\n        "SceneID" : "Scene_sax_video_4OA_audio",\n        "DegreeOfFreedom" : 3,\n        "UnitySceneID" : "_videoPlayer",\n        "VisualStimuli" : "360Video",\n        "VideoFiles" : [\n            {\n                "PathToFile" : "../../Content/video/saxophoneBusking.mp4",\n                "VideoID" : 0\n            }\n        ],\n        "NumberOfAudioFiles" : 1,\n        //highlight-next-line\n        "AudioAmbisonicsFolder" : "../../Content/audio/saxophoneScene/saxophone_4OA.wav",\n        "AudioObjectsFolder" : "<null>",\n        "AudioObjectsrouting" : [ ]\n    }\n]\n'})}),"\n",(0,s.jsx)(n.p,{children:"Here, we've removed information regarding audio objects and object routing, and have inlcuded a path to the Ambisonics audio file."}),"\n",(0,s.jsx)(n.h3,{id:"ambisoncs-audio--cgi",children:"Ambisoncs audio + CGI"}),"\n",(0,s.jsx)(n.p,{children:"An example using a CGI scene and Ambisonics audio"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-JSON",children:'"Scenes" : [\n    {\n        "SceneID" : "Scene_city_1OA_audio",\n        "DegreeOfFreedom" : 3,\n        "UnitySceneID" : "_city",\n        "VisualStimuli" : "simulated",\n        "Dimensions" : [ 50.0, 8.0, 50.0 ],\n        "NumberOfAudioFiles" : 1,\n        "AudioAmbisonicsFolder" : "../../Content/audio/cityScene/cityNoise_1OA.wav",\n        "AudioObjectsFolder" : "<null>",\n        "AudioObjectsrouting" : [ ]\n    }\n]\n'})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}}}]);