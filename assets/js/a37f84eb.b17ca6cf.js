"use strict";(self.webpackChunkqexe_website=self.webpackChunkqexe_website||[]).push([[4417],{986:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>a});var t=i(4848),o=i(8453);const s={sidebar_position:2,custom_edit_url:null,description:"Explore the config file..."},l="The config File",r={id:"getting-started/config-file",title:"The config File",description:"Explore the config file...",source:"@site/docs/getting-started/config-file.md",sourceDirName:"getting-started",slug:"/getting-started/config-file",permalink:"/qexe/docs/getting-started/config-file",draft:!1,unlisted:!1,editUrl:null,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,custom_edit_url:null,description:"Explore the config file..."},sidebar:"tutorialSidebar",previous:{title:"OSC Communication",permalink:"/qexe/docs/qexe-architecture/communication"},next:{title:"Scene Types",permalink:"/qexe/docs/getting-started/scene-types"}},d={},a=[{value:"What&#39;s it for?",id:"whats-it-for",level:4},{value:"How do I use it?",id:"how-do-i-use-it",level:4},{value:"Breakdown",id:"breakdown",level:2},{value:"OSC Connection",id:"osc-connection",level:3},{value:"Test Parameters",id:"test-parameters",level:3},{value:"Content Directory",id:"content-directory",level:3},{value:"Audio Rendering",id:"audio-rendering",level:3},{value:"Scenes",id:"scenes",level:3},{value:"Complete Example",id:"complete-example",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"the-config-file",children:"The config File"}),"\n",(0,t.jsx)(n.h4,{id:"whats-it-for",children:"What's it for?"}),"\n",(0,t.jsx)(n.p,{children:"As the name suggests, the config file is used to configure the QExE Controller. By loading in this file, the QExE Controller will automatically load in the correct material required, calculate the test items, and communicate with the Unity Agent throughout the test to send information."}),"\n",(0,t.jsx)(n.h4,{id:"how-do-i-use-it",children:"How do I use it?"}),"\n",(0,t.jsx)(n.p,{children:'The QExE config file can be loaded into the MaxMSP Controller, buy clicking "Load File", and navigating to your config.json Ffle.'}),"\n",(0,t.jsx)(n.h2,{id:"breakdown",children:"Breakdown"}),"\n",(0,t.jsxs)(n.p,{children:["The following entires fall under the ",(0,t.jsx)(n.code,{children:"testSettings"})," nest of the json File."]}),"\n",(0,t.jsx)(n.h3,{id:"osc-connection",children:"OSC Connection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-JSON",children:'    "udpPortIn" : 8002,\n    "udpPortOut" : 8000,\n    "ip" : "127.0.0.1",\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["We can set the host UDP information via the following settings. An ",(0,t.jsx)(n.code,{children:"ip"})," of 127.0.0.1 will be a local connection on the same PC."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"Technically, any Agent can be connected to the QExE Controller. Therefore, the UDP settings in the Unity Agent must be entered separately. The default UDP settings in the Unity Agent is configured to send and receive messages from the above Controller settings."})}),"\n",(0,t.jsx)(n.h3,{id:"test-parameters",children:"Test Parameters"}),"\n",(0,t.jsx)(n.p,{children:"The following entries will largly dictate how the test items will be calculated..."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-JSON",children:'    "methodType" : "ACR",\n    "questionnaireType" : "NASA-TLX",\n    "questionnaireIntegration" : "Lace",\n    "modalityRatio" : "A",\n    "itemPresentationOder" : "Fixed",\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.a,{href:"../features/methods",children:(0,t.jsx)(n.code,{children:"methodType"})})," is the fundamental base of creating your test. This information loads the correct method code, user interface, and will be used to create the number of unique test items."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.a,{href:"../features/questionnaires",children:(0,t.jsx)(n.code,{children:"questionnaireType"})})," entry can be used to include an optional questionnaire."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"questionnaireIntegration"})," can be used to specify how the questionnaire is added to the test."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{title:"Important!",type:"info",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"methodType"})," and ",(0,t.jsx)(n.code,{children:"questionnaireType"})," string entries will be sent over to the Unity Agent. These string entries are used to search for the corresponding user interface and code in the Unity project."]})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"modalityRatio"})," entry can be used to define a ratio between audio renderers vs. visual content that should be used to calculate the test items, depending on the method employed."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"itemPresentationOrder"})," entry can be used to define if the items should be presented in a ",(0,t.jsx)(n.code,{children:"Fixed"})," or ",(0,t.jsx)(n.code,{children:"Random"})," order."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"content-directory",children:"Content Directory"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-JSON",children:'    "pathToAudioVideoScenesContent" : "D:/user/myProject/theContent",\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"pathToAudioVideoScenesContent"})," entry is used to set a base URL for all of the content used in the ",(0,t.jsx)(n.a,{href:"#scenes",children:"Scene"})," entries. All audio and video files that are specified in the Scene entries will be relative to this folder location."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"audio-rendering",children:"Audio Rendering"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"audioRendering"})," entries dictate if, and how, any audio rendering should be done locally inside the Controller (MaxMSP)."]}),"\n",(0,t.jsx)(n.p,{children:"While the QExE tool does not have native audio rendering, it does allow loading in up to 6 VST plug-ins capable of real-time audio rendering. To load your VSTs, the following entries are used..."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-JSON",children:'"audioRendering" : {\n    "active" : 1,\n    "renderingPipeline" : "Objects",\n    "pathToVSTs" : "D:/work/_QoEVAVE/gitlab_fau/QExE/MaxHost/localtmp",\n    "audioVSTConditions" : [\n        {\n            "conditionID" : "GBRv2.0.2",\n            "vst" : "GBR.dll",\n            "hrtf" : "D2_48K_24bit_256tap_FIR_SOFA.sofa",\n            "vstParameterMap" : "GBRmapper.js"\n        }\n    ]\n}\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"active"})," entry sets of audio rendering should be performed or not."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"renderingPipeline"})," entry specifies which type of audio workflow should be used. This will effect how the audio is loaded into the MaxMSP buffers, and how these buffers are fed to the VSTs.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"renderingPipeline : Objects"})," can take up to 36 mono files for object-based audio rendering."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"renderingPipeline : Multichannel"})," will expect a single multichannel audio file, used for channel or Ambisonics audio renderring (depending on the VST)."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"pathToVSTs"})," entry is used to specify a folder containing your audio VST plug-ins on your local machine."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"audioVSTConditions"})," array is a nest of audio renderers that can be used to render your audio stimuli. These ",(0,t.jsx)(n.code,{children:"audioVSTConditions"})," are analogous to conditions under test in traditional audio evaluation. For example, providing three ",(0,t.jsx)(n.code,{children:"audioVSTConditions"})," in this nest will mean that three audio conditions will be presented in parallel when using methods [multiple stimulus comparison] or [elimination-by-aspects]. If [absolue category rating] method is used, three unique test items will be construstructed per stimuli to reflect the three audio renderers used.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"conditionID"})," entry is a string of your choosing the help identify this condition. The reason we do not just take the .dll name from the following ",(0,t.jsx)(n.code,{children:"vst"})," name is that the same VST may be employed with different settings / or HRTFs."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"vst"})," entry is the name of the VST .dll to load."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"HRTF"})," is the name of the HRTF file you might wish to use inside your VST."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"vstParameterMap"})," is the name of the mapping file used to translate incoming real-time coordinate information from Unity, to the coordinate system of the respective VST."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"scenes",children:"Scenes"}),"\n",(0,t.jsxs)(n.p,{children:["Often referred to as programme material, or stimuli, this is the audiovisual content that will be presented to the subject.\nVisit the ",(0,t.jsx)(n.a,{href:"./eval-scenes",children:"Evaluation Scenes"})," page on a breakdown of the JSON entries used in a Scene."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-JSON",children:'"Scenes" : [\n    {\n        "stimuli_ID" : "InteractionDemo_scene_1",\n        "unityScene_ID" : "_interactionDemo",\n        "visualStimuliType" : "CGI",\n        "dimensions" : [ 30.00, 8.00, 30.0 ],\n        "multichannelAudio" : {\n            "multichannelAudioFile" : null\n        },\n        "objectAudio" : {\n            "objectAudioFolder" : "audio/_bongos/edited",\n            "objectAudioRouting" : [ ] // array of items\n        }\n    },\n    {\n        ....\n    }\n]\n'})}),"\n",(0,t.jsx)(n.h2,{id:"complete-example",children:"Complete Example"}),"\n",(0,t.jsxs)(n.p,{children:["Here's an example config file you can use with the ",(0,t.jsx)(n.code,{children:"_interactionDemo"})," scene provided in the qexe agent unity project."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-JSON",children:'{\n    "testSettings" : {\n        "udpPortIn" : 8002,\n        "udpPortOut" : 8000,\n        "ip" : "127.0.0.1",\n        "methodType" : "ACR",\n        "questionnaireType" : "NASA-TLX",\n        "questionnaireIntegration" : "Lace",\n        "modalityRatio" : "A",\n        "itemPresentationOder" : "Fixed",\n        "pathToAudioVideoScenesContent" : "D:/work/_QoEVAVE/gitlab_fau/QExE/Content",\n        "audioRendering" : {\n            "active" : 1,\n            "renderingPipeline" : "Objects",\n            "pathToVSTs" : "D:/work/_QoEVAVE/gitlab_fau/QExE/MaxHost/localtmp",\n            "audioVSTConditions" : [\n                {\n                    "conditionID" : "GBRv2.0.2",\n                    "vst" : "GBR.dll",\n                    "hrtf" : "D2_48K_24bit_256tap_FIR_SOFA.sofa",\n                    "vstParameterMap" : "GBRmapper.js"\n                }\n            ]\n        }\n    },\n    "Scenes" : [\n        {\n            "stimuli_ID" : "InteractionDemo_scene_1",\n            "unityScene_ID" : "_interactionDemo",\n            "visualStimuliType" : "CGI",\n            "dimensions" : [ 30.00, 8.00, 30.0 ],\n            "multichannelAudio" : {\n                "multichannelAudioFile" : null\n            },\n            "objectAudio" : {\n                "objectAudioFolder" : "audio/_bongos/edited",\n                "objectAudioRouting" : [\n                    {\n                        "pathToFile" : "1_bongos_L.wav",\n                        "inputChannel" : 0,\n                        "autoplay" : 1\n                    },\n                    {\n                        "pathToFile" : "2_bongos_R.wav",\n                        "inputChannel" : 1,\n                        "autoplay" : 1\n                    },\n                    {\n                        "pathToFile" : "3_trainsound_whistle.wav",\n                        "inputChannel" : 2,\n                        "autoplay" : 1\n                    },\n                    {\n                        "pathToFile" : "4_stream.wav",\n                        "inputChannel" : 3,\n                        "autoplay" : 1\n                    },\n                    {\n                        "pathToFile" : "5_birdsong_1.wav",\n                        "inputChannel" : 4,\n                        "autoplay" : 1\n                    },\n                    {\n                        "pathToFile" : "5_brick_2.wav",\n                        "inputChannel" : 5,\n                        "autoplay" : 0\n                    },\n                    {\n                        "pathToFile" : "6_metal_2.wav",\n                        "inputChannel" : 5,\n                        "autoplay" : 0\n                    }\n                ]\n            }\n        }\n    ]\n}\n'})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);